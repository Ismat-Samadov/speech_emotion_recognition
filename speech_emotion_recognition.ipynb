{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition (SER)\n",
    "\n",
    "This notebook builds a Speech Emotion Recognition model using 4 popular datasets:\n",
    "- RAVDESS\n",
    "- CREMA-D\n",
    "- TESS\n",
    "- SAVEE\n",
    "\n",
    "## Emotions Covered:\n",
    "- Angry\n",
    "- Disgust\n",
    "- Fear\n",
    "- Happy\n",
    "- Neutral\n",
    "- Sad\n",
    "- Surprise (in some datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset directory: /Users/ismatsamadov/speech_emotion_recognition/dataset\n",
      "Dataset exists: True\n",
      "\n",
      "✓ Dataset is ready!\n",
      "\n",
      "Dataset contents:\n",
      "  - Crema: 7442 audio files\n",
      "  - Ravdess: 1440 audio files\n",
      "  - Savee: 480 audio files\n",
      "  - Tess: 2800 audio files\n",
      "\n",
      "  Total: 12162 audio files across all datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to the local dataset directory\n",
    "# The dataset has been downloaded and extracted to the 'dataset' folder\n",
    "data_path = os.path.join(os.getcwd(), 'dataset')\n",
    "\n",
    "print(f\"Dataset directory: {data_path}\")\n",
    "print(f\"Dataset exists: {os.path.exists(data_path)}\")\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    print(\"\\n✓ Dataset is ready!\")\n",
    "    print(f\"\\nDataset contents:\")\n",
    "    \n",
    "    # Count files recursively in each subdirectory\n",
    "    subdirs = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
    "    total_files = 0\n",
    "    \n",
    "    for subdir in sorted(subdirs):\n",
    "        subdir_path = os.path.join(data_path, subdir)\n",
    "        # Count all .wav files recursively\n",
    "        num_files = sum(1 for root, dirs, files in os.walk(subdir_path) \n",
    "                       for f in files if f.endswith('.wav'))\n",
    "        total_files += num_files\n",
    "        print(f\"  - {subdir}: {num_files} audio files\")\n",
    "    \n",
    "    print(f\"\\n  Total: {total_files} audio files across all datasets\")\n",
    "else:\n",
    "    print(\"\\n✗ Dataset not found!\")\n",
    "    print(\"Please download the dataset from Kaggle or run the download cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.2\n",
      "Librosa version: 0.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismatsamadov/speech_emotion_recognition/.venv/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Librosa version:\", librosa.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ charts/ directory ready\n",
      "✓ outputs/ directory ready\n",
      "✓ artifacts/ directory ready\n",
      "\n",
      "All output directories created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create output directories\n",
    "import os\n",
    "\n",
    "# Define output directories\n",
    "CHARTS_DIR = 'charts'\n",
    "OUTPUTS_DIR = 'outputs'\n",
    "ARTIFACTS_DIR = 'artifacts'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [CHARTS_DIR, OUTPUTS_DIR, ARTIFACTS_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"✓ {directory}/ directory ready\")\n",
    "\n",
    "print(\"\\nAll output directories created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring dataset structure at: /Users/ismatsamadov/speech_emotion_recognition/dataset\n",
      "\n",
      "================================================================================\n",
      "dataset/\n",
      "\n",
      "  Savee/\n",
      "    └─ 480 .wav files\n",
      "       - JK_sa01.wav\n",
      "       - JK_sa15.wav\n",
      "       - DC_n13.wav\n",
      "       ... and 477 more\n",
      "\n",
      "  Tess/\n",
      "\n",
      "  Ravdess/\n",
      "\n",
      "  Crema/\n",
      "    └─ 7442 .wav files\n",
      "       - 1022_ITS_ANG_XX.wav\n",
      "       - 1037_ITS_ANG_XX.wav\n",
      "       - 1060_ITS_NEU_XX.wav\n",
      "       ... and 7439 more\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Explore dataset directory structure\n",
    "import os\n",
    "\n",
    "# Use the local dataset directory\n",
    "data_path = os.path.join(os.getcwd(), 'dataset')\n",
    "\n",
    "# Find all subdirectories\n",
    "print(f\"Exploring dataset structure at: {data_path}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    # Only show first 2 levels to keep output clean\n",
    "    depth = root.replace(data_path, '').count(os.sep)\n",
    "    if depth < 2:\n",
    "        indent = ' ' * 2 * depth\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        \n",
    "        # Show sample files if any .wav files exist\n",
    "        wav_files = [f for f in files if f.endswith('.wav')]\n",
    "        if wav_files and depth == 1:\n",
    "            print(f\"{indent}  └─ {len(wav_files)} .wav files\")\n",
    "            if len(wav_files) <= 3:\n",
    "                for f in wav_files[:3]:\n",
    "                    print(f\"{indent}     - {f}\")\n",
    "            else:\n",
    "                for f in wav_files[:3]:\n",
    "                    print(f\"{indent}     - {f}\")\n",
    "                print(f\"{indent}     ... and {len(wav_files) - 3} more\")\n",
    "        print()\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Label Extraction\n",
    "\n",
    "We'll create functions to extract emotion labels from each dataset based on their naming conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing emotion extraction:\n",
      "==================================================\n",
      "RAVDESS test: angry\n",
      "CREMA test: angry\n",
      "TESS test: disgust\n",
      "SAVEE tests:\n",
      "  DC_a01.wav: angry\n",
      "  JK_sa15.wav: sad\n",
      "  DC_su01.wav: surprise\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def get_emotion_ravdess(filename):\n",
    "    \"\"\"\n",
    "    Extract emotion from RAVDESS filename\n",
    "    Format: 03-01-{emotion}-01-01-01-01.wav\n",
    "    Emotions: 01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=disgust, 08=surprised\n",
    "    \"\"\"\n",
    "    emotion_map = {\n",
    "        '01': 'neutral',\n",
    "        '02': 'calm',\n",
    "        '03': 'happy',\n",
    "        '04': 'sad',\n",
    "        '05': 'angry',\n",
    "        '06': 'fear',\n",
    "        '07': 'disgust',\n",
    "        '08': 'surprise'\n",
    "    }\n",
    "    parts = os.path.basename(filename).split('-')\n",
    "    if len(parts) >= 3:\n",
    "        return emotion_map.get(parts[2], 'unknown')\n",
    "    return 'unknown'\n",
    "\n",
    "def get_emotion_crema(filename):\n",
    "    \"\"\"\n",
    "    Extract emotion from CREMA-D filename\n",
    "    Format: 1001_DFA_ANG_XX.wav\n",
    "    Emotions: SAD, ANG, DIS, FEA, HAP, NEU\n",
    "    \"\"\"\n",
    "    emotion_map = {\n",
    "        'SAD': 'sad',\n",
    "        'ANG': 'angry',\n",
    "        'DIS': 'disgust',\n",
    "        'FEA': 'fear',\n",
    "        'HAP': 'happy',\n",
    "        'NEU': 'neutral'\n",
    "    }\n",
    "    parts = os.path.basename(filename).split('_')\n",
    "    if len(parts) >= 3:\n",
    "        return emotion_map.get(parts[2], 'unknown')\n",
    "    return 'unknown'\n",
    "\n",
    "def get_emotion_tess(filename):\n",
    "    \"\"\"\n",
    "    Extract emotion from TESS filename\n",
    "    Format: YAF_word_emotion.wav (e.g., YAF_date_disgust.wav)\n",
    "    Emotions are in the filename as the last part before .wav\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filename).lower()\n",
    "    \n",
    "    # Remove .wav extension and split by underscore\n",
    "    name_without_ext = basename.replace('.wav', '')\n",
    "    parts = name_without_ext.split('_')\n",
    "    \n",
    "    # The emotion is typically the last part\n",
    "    if len(parts) >= 3:\n",
    "        emotion = parts[-1]\n",
    "        \n",
    "        # Map variations to standard emotions\n",
    "        emotion_map = {\n",
    "            'angry': 'angry',\n",
    "            'anger': 'angry',\n",
    "            'disgust': 'disgust',\n",
    "            'fear': 'fear',\n",
    "            'happy': 'happy',\n",
    "            'happiness': 'happy',\n",
    "            'pleasant': 'happy',\n",
    "            'ps': 'surprise',  # pleasant surprise\n",
    "            'sad': 'sad',\n",
    "            'sadness': 'sad',\n",
    "            'neutral': 'neutral',\n",
    "            'surprise': 'surprise',\n",
    "            'surprised': 'surprise'\n",
    "        }\n",
    "        \n",
    "        return emotion_map.get(emotion, 'unknown')\n",
    "    \n",
    "    return 'unknown'\n",
    "\n",
    "def get_emotion_savee(filename):\n",
    "    \"\"\"\n",
    "    Extract emotion from SAVEE filename\n",
    "    Format: {SpeakerID}_{EmotionCode}{Number}.wav (e.g., DC_a01.wav, JK_sa15.wav)\n",
    "    SpeakerIDs: DC, JE, JK, KL\n",
    "    Emotions: a=anger, d=disgust, f=fear, h=happiness, n=neutral, sa=sadness, su=surprise\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filename).lower()\n",
    "    \n",
    "    # Split by underscore to separate speaker ID and emotion code\n",
    "    parts = basename.split('_')\n",
    "    \n",
    "    if len(parts) >= 2:\n",
    "        # The second part contains emotion code + number\n",
    "        emotion_part = parts[1].replace('.wav', '')\n",
    "        \n",
    "        # Check for two-letter codes first (sa, su)\n",
    "        if emotion_part.startswith('sa'):\n",
    "            return 'sad'\n",
    "        elif emotion_part.startswith('su'):\n",
    "            return 'surprise'\n",
    "        # Then check single-letter codes\n",
    "        elif emotion_part.startswith('a'):\n",
    "            return 'angry'\n",
    "        elif emotion_part.startswith('d'):\n",
    "            return 'disgust'\n",
    "        elif emotion_part.startswith('f'):\n",
    "            return 'fear'\n",
    "        elif emotion_part.startswith('h'):\n",
    "            return 'happy'\n",
    "        elif emotion_part.startswith('n'):\n",
    "            return 'neutral'\n",
    "    \n",
    "    return 'unknown'\n",
    "\n",
    "def get_emotion(filepath):\n",
    "    \"\"\"\n",
    "    Automatically detect dataset type and extract emotion\n",
    "    \"\"\"\n",
    "    filepath_lower = filepath.lower()\n",
    "    \n",
    "    if 'ravdess' in filepath_lower or 'actor_' in filepath_lower:\n",
    "        return get_emotion_ravdess(filepath)\n",
    "    elif 'crema' in filepath_lower:\n",
    "        return get_emotion_crema(filepath)\n",
    "    elif 'tess' in filepath_lower:\n",
    "        return get_emotion_tess(filepath)\n",
    "    elif 'savee' in filepath_lower:\n",
    "        return get_emotion_savee(filepath)\n",
    "    else:\n",
    "        # Try to infer from filename\n",
    "        if '-' in os.path.basename(filepath) and len(os.path.basename(filepath).split('-')) > 5:\n",
    "            return get_emotion_ravdess(filepath)\n",
    "        elif '_' in os.path.basename(filepath):\n",
    "            # Try CREMA first, then TESS, then SAVEE\n",
    "            emotion = get_emotion_crema(filepath)\n",
    "            if emotion == 'unknown':\n",
    "                emotion = get_emotion_tess(filepath)\n",
    "            if emotion == 'unknown':\n",
    "                emotion = get_emotion_savee(filepath)\n",
    "            return emotion\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    \n",
    "# Test the functions\n",
    "print(\"Testing emotion extraction:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"RAVDESS test:\", get_emotion_ravdess(\"03-01-05-01-01-01-01.wav\"))\n",
    "print(\"CREMA test:\", get_emotion_crema(\"1001_DFA_ANG_XX.wav\"))\n",
    "print(\"TESS test:\", get_emotion_tess(\"YAF_date_disgust.wav\"))\n",
    "print(\"SAVEE tests:\")\n",
    "print(\"  DC_a01.wav:\", get_emotion_savee(\"DC_a01.wav\"))\n",
    "print(\"  JK_sa15.wav:\", get_emotion_savee(\"JK_sa15.wav\"))\n",
    "print(\"  DC_su01.wav:\", get_emotion_savee(\"DC_su01.wav\"))\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "We'll extract multiple audio features:\n",
    "- MFCC (Mel-frequency cepstral coefficients)\n",
    "- Chroma\n",
    "- Mel Spectrogram\n",
    "- Zero Crossing Rate\n",
    "- Spectral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction function ready.\n"
     ]
    }
   ],
   "source": [
    "def extract_features(file_path, duration=3, sr=22050):\n",
    "    \"\"\"\n",
    "    Extract comprehensive audio features from audio file\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: path to audio file\n",
    "    - duration: maximum duration to load (in seconds)\n",
    "    - sr: sampling rate\n",
    "    \n",
    "    Returns:\n",
    "    - features: numpy array of concatenated features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        audio, sample_rate = librosa.load(file_path, duration=duration, sr=sr)\n",
    "        \n",
    "        # Extract MFCC features (40 coefficients)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "        \n",
    "        # Extract Chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "        chroma_mean = np.mean(chroma.T, axis=0)\n",
    "        \n",
    "        # Extract Mel Spectrogram\n",
    "        mel = librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n",
    "        mel_mean = np.mean(mel.T, axis=0)\n",
    "        \n",
    "        # Extract Spectral Contrast\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=sample_rate)\n",
    "        contrast_mean = np.mean(contrast.T, axis=0)\n",
    "        \n",
    "        # Extract Tonnetz (Tonal Centroid Features)\n",
    "        tonnetz = librosa.feature.tonnetz(y=audio, sr=sample_rate)\n",
    "        tonnetz_mean = np.mean(tonnetz.T, axis=0)\n",
    "        \n",
    "        # Extract Zero Crossing Rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio)\n",
    "        zcr_mean = np.mean(zcr)\n",
    "        \n",
    "        # Extract Spectral Rolloff\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sample_rate)\n",
    "        rolloff_mean = np.mean(spectral_rolloff)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        features = np.hstack([\n",
    "            mfccs_mean,\n",
    "            chroma_mean,\n",
    "            mel_mean,\n",
    "            contrast_mean,\n",
    "            tonnetz_mean,\n",
    "            zcr_mean,\n",
    "            rolloff_mean\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Feature extraction function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load All Audio Files and Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total audio files found: 12162\n",
      "\n",
      "Sample file paths:\n",
      "1. /Users/ismatsamadov/speech_emotion_recognition/dataset/Savee/JK_sa01.wav\n",
      "   Detected emotion: sad\n",
      "2. /Users/ismatsamadov/speech_emotion_recognition/dataset/Savee/JK_sa15.wav\n",
      "   Detected emotion: sad\n",
      "3. /Users/ismatsamadov/speech_emotion_recognition/dataset/Savee/DC_n13.wav\n",
      "   Detected emotion: neutral\n",
      "4. /Users/ismatsamadov/speech_emotion_recognition/dataset/Savee/DC_su09.wav\n",
      "   Detected emotion: surprise\n",
      "5. /Users/ismatsamadov/speech_emotion_recognition/dataset/Savee/DC_n07.wav\n",
      "   Detected emotion: neutral\n"
     ]
    }
   ],
   "source": [
    "# Collect all .wav files from the dataset\n",
    "audio_files = []\n",
    "for root, dirs, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            audio_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Total audio files found: {len(audio_files)}\")\n",
    "\n",
    "# Display sample file paths\n",
    "if len(audio_files) > 0:\n",
    "    print(\"\\nSample file paths:\")\n",
    "    for i in range(min(5, len(audio_files))):\n",
    "        print(f\"{i+1}. {audio_files[i]}\")\n",
    "        print(f\"   Detected emotion: {get_emotion(audio_files[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from all audio files...\n",
      "This may take several minutes depending on the dataset size.\n",
      "\n",
      "Processing: 100/12162 files (0.8%)\n",
      "Processing: 200/12162 files (1.6%)\n",
      "Processing: 300/12162 files (2.5%)\n",
      "Processing: 400/12162 files (3.3%)\n",
      "Processing: 500/12162 files (4.1%)\n",
      "Processing: 600/12162 files (4.9%)\n",
      "Processing: 700/12162 files (5.8%)\n",
      "Processing: 800/12162 files (6.6%)\n",
      "Processing: 900/12162 files (7.4%)\n",
      "Processing: 1000/12162 files (8.2%)\n",
      "Processing: 1100/12162 files (9.0%)\n",
      "Processing: 1200/12162 files (9.9%)\n",
      "Processing: 1300/12162 files (10.7%)\n",
      "Processing: 1400/12162 files (11.5%)\n",
      "Processing: 1500/12162 files (12.3%)\n",
      "Processing: 1600/12162 files (13.2%)\n",
      "Processing: 1700/12162 files (14.0%)\n",
      "Processing: 1800/12162 files (14.8%)\n",
      "Processing: 1900/12162 files (15.6%)\n",
      "Processing: 2000/12162 files (16.4%)\n",
      "Processing: 2100/12162 files (17.3%)\n",
      "Processing: 2200/12162 files (18.1%)\n",
      "Processing: 2300/12162 files (18.9%)\n",
      "Processing: 2400/12162 files (19.7%)\n",
      "Processing: 2500/12162 files (20.6%)\n",
      "Processing: 2600/12162 files (21.4%)\n",
      "Processing: 2700/12162 files (22.2%)\n",
      "Processing: 2800/12162 files (23.0%)\n",
      "Processing: 2900/12162 files (23.8%)\n",
      "Processing: 3000/12162 files (24.7%)\n",
      "Processing: 3100/12162 files (25.5%)\n",
      "Processing: 3200/12162 files (26.3%)\n",
      "Processing: 3300/12162 files (27.1%)\n",
      "Processing: 3400/12162 files (28.0%)\n",
      "Processing: 3500/12162 files (28.8%)\n",
      "Processing: 3600/12162 files (29.6%)\n",
      "Processing: 3700/12162 files (30.4%)\n",
      "Processing: 3800/12162 files (31.2%)\n",
      "Processing: 3900/12162 files (32.1%)\n",
      "Processing: 4000/12162 files (32.9%)\n",
      "Processing: 4100/12162 files (33.7%)\n",
      "Processing: 4200/12162 files (34.5%)\n",
      "Processing: 4300/12162 files (35.4%)\n",
      "Processing: 4400/12162 files (36.2%)\n",
      "Processing: 4500/12162 files (37.0%)\n",
      "Processing: 4600/12162 files (37.8%)\n",
      "Processing: 4700/12162 files (38.6%)\n",
      "Processing: 4800/12162 files (39.5%)\n",
      "Processing: 4900/12162 files (40.3%)\n",
      "Processing: 5000/12162 files (41.1%)\n",
      "Processing: 5100/12162 files (41.9%)\n",
      "Processing: 5200/12162 files (42.8%)\n",
      "Processing: 5300/12162 files (43.6%)\n",
      "Processing: 5400/12162 files (44.4%)\n",
      "Processing: 5500/12162 files (45.2%)\n",
      "Processing: 5600/12162 files (46.0%)\n",
      "Processing: 5700/12162 files (46.9%)\n",
      "Processing: 5800/12162 files (47.7%)\n",
      "Processing: 5900/12162 files (48.5%)\n",
      "Processing: 6000/12162 files (49.3%)\n",
      "Processing: 6100/12162 files (50.2%)\n",
      "Processing: 6200/12162 files (51.0%)\n",
      "Processing: 6300/12162 files (51.8%)\n",
      "Processing: 6400/12162 files (52.6%)\n",
      "Processing: 6500/12162 files (53.4%)\n",
      "Processing: 6600/12162 files (54.3%)\n",
      "Processing: 6700/12162 files (55.1%)\n",
      "Processing: 6800/12162 files (55.9%)\n",
      "Processing: 6900/12162 files (56.7%)\n",
      "Processing: 7000/12162 files (57.6%)\n",
      "Processing: 7100/12162 files (58.4%)\n",
      "Processing: 7200/12162 files (59.2%)\n",
      "Processing: 7300/12162 files (60.0%)\n",
      "Processing: 7400/12162 files (60.8%)\n",
      "Processing: 7500/12162 files (61.7%)\n",
      "Processing: 7600/12162 files (62.5%)\n",
      "Processing: 7700/12162 files (63.3%)\n",
      "Processing: 7800/12162 files (64.1%)\n",
      "Processing: 7900/12162 files (65.0%)\n",
      "Processing: 8000/12162 files (65.8%)\n"
     ]
    }
   ],
   "source": [
    "# Extract features and labels for all audio files\n",
    "print(\"Extracting features from all audio files...\")\n",
    "print(\"This may take several minutes depending on the dataset size.\\n\")\n",
    "\n",
    "features_list = []\n",
    "labels_list = []\n",
    "file_paths = []\n",
    "\n",
    "# Process files with progress indication\n",
    "total_files = len(audio_files)\n",
    "for idx, file_path in enumerate(audio_files, 1):\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processing: {idx}/{total_files} files ({idx/total_files*100:.1f}%)\")\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(file_path)\n",
    "    \n",
    "    if features is not None:\n",
    "        # Get emotion label\n",
    "        emotion = get_emotion(file_path)\n",
    "        \n",
    "        if emotion != 'unknown':\n",
    "            features_list.append(features)\n",
    "            labels_list.append(emotion)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "print(f\"\\nFeature extraction complete!\")\n",
    "print(f\"Successfully processed: {len(features_list)} files\")\n",
    "print(f\"Skipped: {total_files - len(features_list)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for easier analysis\n",
    "df = pd.DataFrame({\n",
    "    'file_path': file_paths,\n",
    "    'emotion': labels_list\n",
    "})\n",
    "\n",
    "# Add dataset source column\n",
    "df['dataset'] = df['file_path'].apply(lambda x: \n",
    "    'RAVDESS' if 'ravdess' in x.lower() else\n",
    "    'CREMA' if 'crema' in x.lower() else\n",
    "    'TESS' if 'tess' in x.lower() else\n",
    "    'SAVEE' if 'savee' in x.lower() else 'Unknown'\n",
    ")\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nEmotion distribution:\")\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "print(emotion_counts)\n",
    "\n",
    "# Save emotion distribution to outputs\n",
    "emotion_counts.to_csv(f'{OUTPUTS_DIR}/emotion_distribution.csv')\n",
    "print(f\"\\n✓ Saved emotion distribution to {OUTPUTS_DIR}/emotion_distribution.csv\")\n",
    "\n",
    "# Dataset distribution\n",
    "print(f\"\\nDataset distribution:\")\n",
    "dataset_counts = df['dataset'].value_counts()\n",
    "print(dataset_counts)\n",
    "dataset_counts.to_csv(f'{OUTPUTS_DIR}/dataset_distribution.csv')\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nFeature vector shape: {features_list[0].shape}\")\n",
    "print(f\"Number of features per sample: {len(features_list[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# 1. Emotion Distribution\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "emotion_counts = df['emotion'].value_counts()\n",
    "bars = ax.bar(emotion_counts.index, emotion_counts.values, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.set_title('Emotion Distribution Across All Datasets', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Emotion', fontsize=12)\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/01_emotion_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/01_emotion_distribution.png\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Dataset Distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "dataset_counts = df['dataset'].value_counts()\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "wedges, texts, autotexts = ax.pie(dataset_counts.values, labels=dataset_counts.index, autopct='%1.1f%%',\n",
    "                                     colors=colors, startangle=90, explode=[0.05]*len(dataset_counts))\n",
    "ax.set_title('Dataset Source Distribution', fontsize=16, fontweight='bold')\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "    autotext.set_fontsize(11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/02_dataset_distribution_pie.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/02_dataset_distribution_pie.png\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Emotion Distribution by Dataset (Stacked Bar Chart)\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "emotion_by_dataset = pd.crosstab(df['dataset'], df['emotion'])\n",
    "emotion_by_dataset.plot(kind='bar', stacked=True, ax=ax, colormap='tab10', edgecolor='black')\n",
    "ax.set_title('Emotion Distribution by Dataset Source', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.legend(title='Emotion', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/03_emotion_by_dataset_stacked.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/03_emotion_by_dataset_stacked.png\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Heatmap of Emotion Distribution by Dataset\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.heatmap(emotion_by_dataset.T, annot=True, fmt='d', cmap='YlOrRd', ax=ax, cbar_kws={'label': 'Count'})\n",
    "ax.set_title('Emotion-Dataset Heatmap', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Emotion', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/04_emotion_dataset_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/04_emotion_dataset_heatmap.png\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Emotion Distribution Comparison (Grouped Bar Chart)\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "emotion_by_dataset.plot(kind='bar', ax=ax, colormap='Set3', edgecolor='black', width=0.8)\n",
    "ax.set_title('Emotion Counts by Dataset (Grouped)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Dataset', fontsize=12)\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.legend(title='Emotion', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/05_emotion_by_dataset_grouped.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/05_emotion_by_dataset_grouped.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ All initial charts saved to '{CHARTS_DIR}/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Preparation for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "X = np.array(features_list)\n",
    "y = np.array(labels_list)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "print(f\"\\nEncoded labels shape: {y_categorical.shape}\")\n",
    "print(f\"Number of emotion classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"Emotion classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_categorical, test_size=0.3, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nData standardization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a deep neural network for speech emotion recognition\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Input layer\n",
    "        layers.Dense(512, activation='relu', input_shape=(input_shape,)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Hidden layers\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "num_features = X_train_scaled.shape[1]\n",
    "num_classes = y_categorical.shape[1]\n",
    "\n",
    "model = create_model(num_features, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    f'{ARTIFACTS_DIR}/best_ser_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"✓ Best model saved to {ARTIFACTS_DIR}/best_ser_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    'best_ser_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "axes[0].set_title('Model Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2, marker='o', markersize=4)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "axes[1].set_title('Model Loss Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/06_training_history.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/06_training_history.png\")\n",
    "plt.show()\n",
    "\n",
    "# Save training history to CSV\n",
    "history_df = pd.DataFrame({\n",
    "    'epoch': range(1, len(history.history['loss']) + 1),\n",
    "    'train_loss': history.history['loss'],\n",
    "    'val_loss': history.history['val_loss'],\n",
    "    'train_accuracy': history.history['accuracy'],\n",
    "    'val_accuracy': history.history['val_accuracy']\n",
    "})\n",
    "history_df.to_csv(f'{OUTPUTS_DIR}/training_history.csv', index=False)\n",
    "print(f\"✓ Saved training history to {OUTPUTS_DIR}/training_history.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[0].set_title('Model Accuracy', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss\n",
    "axes[1].plot(history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[1].set_title('Model Loss', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "report = classification_report(\n",
    "    y_test_classes,\n",
    "    y_pred_classes,\n",
    "    target_names=label_encoder.classes_,\n",
    "    output_dict=True\n",
    ")\n",
    "print(classification_report(\n",
    "    y_test_classes,\n",
    "    y_pred_classes,\n",
    "    target_names=label_encoder.classes_\n",
    "))\n",
    "\n",
    "# Save classification report to CSV\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df.to_csv(f'{OUTPUTS_DIR}/classification_report.csv')\n",
    "print(f\"\\n✓ Saved classification report to {OUTPUTS_DIR}/classification_report.csv\")\n",
    "\n",
    "# Save test results summary\n",
    "test_results = {\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_loss': test_loss,\n",
    "    'num_test_samples': len(y_test),\n",
    "    'num_classes': len(label_encoder.classes_),\n",
    "    'classes': list(label_encoder.classes_)\n",
    "}\n",
    "import json\n",
    "with open(f'{OUTPUTS_DIR}/test_results.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=4)\n",
    "print(f\"✓ Saved test results to {OUTPUTS_DIR}/test_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(\n",
    "    y_test_classes,\n",
    "    y_pred_classes,\n",
    "    target_names=label_encoder.classes_\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_,\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Count'},\n",
    "    annot_kws={'fontsize': 11}\n",
    ")\n",
    "ax.set_title('Confusion Matrix - Speech Emotion Recognition', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Emotion', fontsize=13)\n",
    "ax.set_ylabel('True Emotion', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/07_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/07_confusion_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "# Save confusion matrix to CSV\n",
    "cm_df = pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
    "cm_df.to_csv(f'{OUTPUTS_DIR}/confusion_matrix.csv')\n",
    "print(f\"✓ Saved confusion matrix to {OUTPUTS_DIR}/confusion_matrix.csv\")\n",
    "\n",
    "# Calculate and display per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "print(\"=\"*40)\n",
    "per_class_acc = {}\n",
    "for i, emotion in enumerate(label_encoder.classes_):\n",
    "    class_accuracy = cm[i, i] / cm[i].sum() * 100\n",
    "    per_class_acc[emotion] = class_accuracy\n",
    "    print(f\"{emotion.capitalize()}: {class_accuracy:.2f}%\")\n",
    "\n",
    "# Save per-class accuracy\n",
    "per_class_df = pd.DataFrame(list(per_class_acc.items()), columns=['Emotion', 'Accuracy'])\n",
    "per_class_df.to_csv(f'{OUTPUTS_DIR}/per_class_accuracy.csv', index=False)\n",
    "print(f\"\\n✓ Saved per-class accuracy to {OUTPUTS_DIR}/per_class_accuracy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Performance Visualizations\n",
    "\n",
    "# Recalculate per-class accuracy (in case not already defined)\n",
    "per_class_acc = {}\n",
    "for i, emotion in enumerate(label_encoder.classes_):\n",
    "    class_accuracy = cm[i, i] / cm[i].sum() * 100\n",
    "    per_class_acc[emotion] = class_accuracy\n",
    "\n",
    "# 1. Per-Class Accuracy Bar Chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "emotions = list(per_class_acc.keys())\n",
    "accuracies = list(per_class_acc.values())\n",
    "colors = ['#2ecc71' if acc >= 60 else '#e74c3c' for acc in accuracies]\n",
    "bars = ax.bar(emotions, accuracies, color=colors, edgecolor='black', alpha=0.7)\n",
    "ax.set_title('Per-Class Accuracy', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Emotion', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.axhline(y=test_accuracy*100, color='blue', linestyle='--', linewidth=2, label=f'Overall Accuracy: {test_accuracy*100:.2f}%')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/08_per_class_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/08_per_class_accuracy.png\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Normalized Confusion Matrix (Percentages)\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='RdYlGn',\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_,\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Percentage (%)'},\n",
    "    annot_kws={'fontsize': 10},\n",
    "    vmin=0,\n",
    "    vmax=100\n",
    ")\n",
    "ax.set_title('Normalized Confusion Matrix (Percentage)', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Emotion', fontsize=13)\n",
    "ax.set_ylabel('True Emotion', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/09_confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/09_confusion_matrix_normalized.png\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Precision, Recall, F1-Score Comparison\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test_classes, y_pred_classes, labels=range(len(label_encoder.classes_))\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = np.arange(len(label_encoder.classes_))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, precision * 100, width, label='Precision', color='#3498db', edgecolor='black')\n",
    "bars2 = ax.bar(x, recall * 100, width, label='Recall', color='#2ecc71', edgecolor='black')\n",
    "bars3 = ax.bar(x + width, f1 * 100, width, label='F1-Score', color='#e74c3c', edgecolor='black')\n",
    "\n",
    "ax.set_title('Precision, Recall, and F1-Score by Emotion', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Emotion', fontsize=12)\n",
    "ax.set_ylabel('Score (%)', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(label_encoder.classes_, rotation=45)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/10_precision_recall_f1.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/10_precision_recall_f1.png\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Support (Sample Count) by Emotion\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.bar(label_encoder.classes_, support, color='#9b59b6', edgecolor='black', alpha=0.7)\n",
    "ax.set_title('Test Set Sample Distribution by Emotion', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Emotion', fontsize=12)\n",
    "ax.set_ylabel('Number of Samples', fontsize=12)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{CHARTS_DIR}/11_test_set_distribution.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved: {CHARTS_DIR}/11_test_set_distribution.png\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ All performance charts saved to '{CHARTS_DIR}/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "# Save the final model\n",
    "model_path = f'{ARTIFACTS_DIR}/speech_emotion_recognition_model.keras'\n",
    "model.save(model_path)\n",
    "print(f\"✓ Model saved: {model_path}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = f'{ARTIFACTS_DIR}/scaler.pkl'\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"✓ Scaler saved: {scaler_path}\")\n",
    "\n",
    "# Save the label encoder\n",
    "label_encoder_path = f'{ARTIFACTS_DIR}/label_encoder.pkl'\n",
    "with open(label_encoder_path, 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(f\"✓ Label encoder saved: {label_encoder_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_architecture': 'Sequential DNN',\n",
    "    'input_features': int(num_features),\n",
    "    'num_classes': int(num_classes),\n",
    "    'emotion_classes': list(label_encoder.classes_),\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'test_loss': float(test_loss),\n",
    "    'total_parameters': int(model.count_params()),\n",
    "    'training_samples': int(X_train.shape[0]),\n",
    "    'validation_samples': int(X_val.shape[0]),\n",
    "    'test_samples': int(X_test.shape[0]),\n",
    "    'feature_extraction': {\n",
    "        'MFCC': 40,\n",
    "        'Chroma': 12,\n",
    "        'Mel_Spectrogram': 128,\n",
    "        'Spectral_Contrast': 7,\n",
    "        'Tonnetz': 6,\n",
    "        'ZCR': 1,\n",
    "        'Spectral_Rolloff': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = f'{ARTIFACTS_DIR}/model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "print(f\"✓ Model metadata saved: {metadata_path}\")\n",
    "\n",
    "print(f\"\\n✓ All model artifacts saved to '{ARTIFACTS_DIR}/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Model and Preprocessing Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the final model\n",
    "model.save('speech_emotion_recognition_model.keras')\n",
    "print(\"Model saved as 'speech_emotion_recognition_model.keras'\")\n",
    "\n",
    "# Save the scaler\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Scaler saved as 'scaler.pkl'\")\n",
    "\n",
    "# Save the label encoder\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "print(\"Label encoder saved as 'label_encoder.pkl'\")\n",
    "\n",
    "print(\"\\nAll model artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Prediction Function for New Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio_and_prediction(audio_file_path, model, scaler, label_encoder, save_chart=True):\n",
    "    \"\"\"\n",
    "    Visualize audio waveform, spectrogram, and emotion prediction\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(audio_file_path, duration=3)\n",
    "    \n",
    "    # Get prediction\n",
    "    predicted_emotion, probabilities = predict_emotion(\n",
    "        audio_file_path, model, scaler, label_encoder\n",
    "    )\n",
    "    actual_emotion = get_emotion(audio_file_path)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Waveform\n",
    "    axes[0, 0].set_title('Waveform', fontsize=12, fontweight='bold')\n",
    "    librosa.display.waveshow(y, sr=sr, ax=axes[0, 0], color='#3498db')\n",
    "    axes[0, 0].set_xlabel('Time (s)')\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[0, 1], cmap='viridis')\n",
    "    axes[0, 1].set_title('Spectrogram', fontsize=12, fontweight='bold')\n",
    "    fig.colorbar(img, ax=axes[0, 1], format='%+2.0f dB')\n",
    "    \n",
    "    # MFCC\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    img2 = librosa.display.specshow(mfccs, sr=sr, x_axis='time', ax=axes[1, 0], cmap='coolwarm')\n",
    "    axes[1, 0].set_title('MFCC', fontsize=12, fontweight='bold')\n",
    "    fig.colorbar(img2, ax=axes[1, 0])\n",
    "    \n",
    "    # Prediction probabilities\n",
    "    emotions = list(probabilities.keys())\n",
    "    probs = list(probabilities.values())\n",
    "    colors = ['#2ecc71' if e == predicted_emotion else '#3498db' for e in emotions]\n",
    "    \n",
    "    axes[1, 1].barh(emotions, probs, color=colors, edgecolor='black', alpha=0.8)\n",
    "    axes[1, 1].set_xlabel('Probability (%)', fontsize=10)\n",
    "    axes[1, 1].set_title(\n",
    "        f'Predicted: {predicted_emotion.upper()} | Actual: {actual_emotion.upper()}',\n",
    "        fontsize=12,\n",
    "        fontweight='bold',\n",
    "        color='green' if predicted_emotion == actual_emotion else 'red'\n",
    "    )\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "    axes[1, 1].set_xlim([0, 100])\n",
    "    \n",
    "    plt.suptitle(\n",
    "        f'Audio Analysis: {os.path.basename(audio_file_path)}',\n",
    "        fontsize=14,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_chart:\n",
    "        chart_filename = f'{CHARTS_DIR}/12_audio_sample_{os.path.basename(audio_file_path).replace(\".wav\", \"\")}.png'\n",
    "        plt.savefig(chart_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: {chart_filename}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize multiple samples\n",
    "print(\"Visualizing sample audio files...\\n\")\n",
    "sample_indices = [10, 100, 500, 1000] if len(file_paths) > 1000 else [10, 50, 100]\n",
    "\n",
    "for idx in sample_indices:\n",
    "    if idx < len(file_paths):\n",
    "        print(f\"\\nSample {idx}:\")\n",
    "        visualize_audio_and_prediction(file_paths[idx], model, scaler, label_encoder, save_chart=True)\n",
    "\n",
    "print(f\"\\n✓ Audio visualization charts saved to '{CHARTS_DIR}/' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Visualize Sample Audio and Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully:\n",
    "\n",
    "1. **Downloaded and processed** 4 speech emotion datasets (RAVDESS, CREMA-D, TESS, SAVEE)\n",
    "2. **Extracted comprehensive audio features** (MFCC, Chroma, Mel Spectrogram, Spectral Contrast, Tonnetz, ZCR, Spectral Rolloff)\n",
    "3. **Built and trained** a deep neural network for emotion classification\n",
    "4. **Achieved** ~64% test accuracy across 8 emotion classes\n",
    "5. **Generated visualizations** and saved all outputs to organized directories\n",
    "\n",
    "### Output Organization\n",
    "\n",
    "- **`/charts`** - All visualization charts (12+ charts)\n",
    "- **`/outputs`** - Data outputs and metrics (CSVs, JSONs)\n",
    "- **`/artifacts`** - Trained models and preprocessing objects\n",
    "\n",
    "The model can now predict emotions from speech audio files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SPEECH EMOTION RECOGNITION - OUTPUT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Charts\n",
    "print(f\"\\n📊 CHARTS ({CHARTS_DIR}/)\")\n",
    "print(\"-\" * 80)\n",
    "chart_files = sorted(glob.glob(f'{CHARTS_DIR}/*.png'))\n",
    "for i, chart in enumerate(chart_files, 1):\n",
    "    print(f\"  {i:2d}. {os.path.basename(chart)}\")\n",
    "\n",
    "# Outputs\n",
    "print(f\"\\n📄 OUTPUTS ({OUTPUTS_DIR}/)\")\n",
    "print(\"-\" * 80)\n",
    "output_files = sorted(glob.glob(f'{OUTPUTS_DIR}/*'))\n",
    "for i, output in enumerate(output_files, 1):\n",
    "    file_size = os.path.getsize(output) / 1024  # KB\n",
    "    print(f\"  {i}. {os.path.basename(output):40s} ({file_size:>8.2f} KB)\")\n",
    "\n",
    "# Artifacts\n",
    "print(f\"\\n🎯 ARTIFACTS ({ARTIFACTS_DIR}/)\")\n",
    "print(\"-\" * 80)\n",
    "artifact_files = sorted(glob.glob(f'{ARTIFACTS_DIR}/*'))\n",
    "for i, artifact in enumerate(artifact_files, 1):\n",
    "    file_size = os.path.getsize(artifact) / (1024 * 1024)  # MB\n",
    "    print(f\"  {i}. {os.path.basename(artifact):40s} ({file_size:>8.2f} MB)\")\n",
    "\n",
    "# Summary statistics\n",
    "total_charts = len(chart_files)\n",
    "total_outputs = len(output_files)\n",
    "total_artifacts = len(artifact_files)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOTALS:\")\n",
    "print(f\"  📊 Charts:    {total_charts}\")\n",
    "print(f\"  📄 Outputs:   {total_outputs}\")\n",
    "print(f\"  🎯 Artifacts: {total_artifacts}\")\n",
    "print(f\"  📁 Total:     {total_charts + total_outputs + total_artifacts} files\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Save summary to file\n",
    "summary_path = f'{OUTPUTS_DIR}/file_summary.txt'\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"SPEECH EMOTION RECOGNITION - OUTPUT SUMMARY\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    f.write(f\"Charts: {total_charts}\\n\")\n",
    "    f.write(f\"Outputs: {total_outputs}\\n\")\n",
    "    f.write(f\"Artifacts: {total_artifacts}\\n\")\n",
    "    f.write(f\"\\nGenerated on: {pd.Timestamp.now()}\\n\")\n",
    "\n",
    "print(f\"\\n✓ Summary report saved to {summary_path}\")\n",
    "print(f\"\\n✅ All outputs successfully organized and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio_and_prediction(audio_file_path, model, scaler, label_encoder):\n",
    "    \"\"\"\n",
    "    Visualize audio waveform, spectrogram, and emotion prediction\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(audio_file_path, duration=3)\n",
    "    \n",
    "    # Get prediction\n",
    "    predicted_emotion, probabilities = predict_emotion(\n",
    "        audio_file_path, model, scaler, label_encoder\n",
    "    )\n",
    "    actual_emotion = get_emotion(audio_file_path)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Waveform\n",
    "    axes[0, 0].set_title('Waveform', fontsize=12)\n",
    "    librosa.display.waveshow(y, sr=sr, ax=axes[0, 0])\n",
    "    axes[0, 0].set_xlabel('Time')\n",
    "    axes[0, 0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Spectrogram', fontsize=12)\n",
    "    fig.colorbar(img, ax=axes[0, 1], format='%+2.0f dB')\n",
    "    \n",
    "    # MFCC\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    img2 = librosa.display.specshow(mfccs, sr=sr, x_axis='time', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('MFCC', fontsize=12)\n",
    "    fig.colorbar(img2, ax=axes[1, 0])\n",
    "    \n",
    "    # Prediction probabilities\n",
    "    emotions = list(probabilities.keys())\n",
    "    probs = list(probabilities.values())\n",
    "    colors = ['green' if e == predicted_emotion else 'blue' for e in emotions]\n",
    "    \n",
    "    axes[1, 1].barh(emotions, probs, color=colors, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Probability (%)', fontsize=10)\n",
    "    axes[1, 1].set_title(\n",
    "        f'Predicted: {predicted_emotion} | Actual: {actual_emotion}',\n",
    "        fontsize=12,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.suptitle(\n",
    "        f'Audio Analysis: {os.path.basename(audio_file_path)}',\n",
    "        fontsize=14,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize a sample\n",
    "if len(file_paths) > 0:\n",
    "    sample_file = file_paths[10] if len(file_paths) > 10 else file_paths[0]\n",
    "    visualize_audio_and_prediction(sample_file, model, scaler, label_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
